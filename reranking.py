import polars as pl
import pandas as pd
import numpy as np
import random
from tqdm import tqdm
# [NEW] Import LightGBM
import lightgbm as lgb 
from sklearn.linear_model import Lasso
import pickle 

# --- 1. SAMPLING FUNCTIONS (Giá»¯ nguyÃªn) ---
def positive_sampling(transaction_lf, q_val):
    print(">> Positive Sampling (Ground Truth)...")
    val_lf = transaction_lf.filter(pl.sql_expr(q_val))
    val_cust_ids = val_lf.select("customer_id").unique().collect()["customer_id"].to_list()
    
    pos_df = (
        val_lf
        .select(["customer_id", "item_id"])
        .unique()
        .with_columns(pl.lit(1, dtype=pl.Int64).alias("target"))
        .collect()
    )
    return pos_df, val_cust_ids
def negative_sampling(candidates_df, pos_df, cfg):
    """
    Fast & safe negative sampling (Polars)
    Láº¥y máº«u Negative tá»« chÃ­nh táº­p Candidates (Hard Negatives)
    thay vÃ¬ láº¥y ngáº«u nhiÃªn toÃ n sÃ n.
    """
    print(">> Negative Sampling (From Candidates)...")
    
    # 1. Chuáº©n bá»‹ Positives (Chá»‰ cáº§n 2 cá»™t Ä‘á»ƒ Anti-Join)
    # Äáº£m báº£o pos_df Ä‘Ã£ collect (lÃ  DataFrame)
    pos_keys = pos_df.select(["customer_id", "item_id"])

    # 2. Oversample tá»« Candidates
    # Láº¥y gáº¥p 3 láº§n n_neg Ä‘á»ƒ sau khi trá»« Ä‘i Positive váº«n cÃ²n Ä‘á»§
    n_sample = cfg["n_neg"] * 3
    
    # [Tá»I Æ¯U] Náº¿u candidates quÃ¡ lá»›n, sample trÆ°á»›c khi join Ä‘á»ƒ nháº¹ RAM
    sampled_lf = (
        candidates_df.lazy()  # Xá»­ lÃ½ Lazy Ä‘á»ƒ tá»‘i Æ°u
        .group_by("customer_id")
        .agg(
            pl.col("item_id").sample(
                n=n_sample, 
                with_replacement=True, # Cho phÃ©p láº·p náº¿u candidate < n_sample
                shuffle=True
            )
        )
        .explode("item_id")
    )

    # 3. Loáº¡i bá» nhá»¯ng mÃ³n KHÃCH ÄÃƒ MUA (Positives)
    # Anti-join: Giá»¯ láº¡i nhá»¯ng mÃ³n trong Candidates mÃ  KHÃ”NG náº±m trong Positives
    # ÄÃ¢y chÃ­nh lÃ  nhá»¯ng mÃ³n "Model gá»£i Ã½ nhÆ°ng KhÃ¡ch khÃ´ng mua" (Hard Negatives)
    neg_df = (
        sampled_lf.collect() # Collect vá» RAM Ä‘á»ƒ join
        .join(
            pos_keys, 
            on=["customer_id", "item_id"], 
            how="anti"
        )
    )

    # 4. Downsample vá» Ä‘Ãºng sá»‘ lÆ°á»£ng n_neg cáº§n thiáº¿t
    final_neg_df = (
        neg_df
        .group_by("customer_id")
        .agg(
            pl.col("item_id").sample(
                n=cfg["n_neg"], 
                with_replacement=False,
                shuffle=True
            )
        )
        .explode("item_id")
        .with_columns(pl.lit(0, dtype=pl.Int64).alias("target")) # GÃ¡n nhÃ£n 0
        .select(["customer_id", "item_id", "target"])
    )
    
    return final_neg_df



# def negative_sampling(transaction_lf, q_val, val_cust_ids, cfg):
#     print(">> Negative Sampling (Random opular items)...")
#     n_neg = cfg["n_neg"]
#     n_top = cfg["top_n"]
    
#     # Láº¥y Top Items
#     top_items = (
#         transaction_lf
#         .filter(pl.sql_expr(q_val))
#         .group_by("item_id")
#         .agg(pl.col("created_date").len().alias("cnt"))
#         .sort("cnt", descending=True)
#         .limit(n_top)
#         .select("item_id")
#         .collect()
#         ["item_id"].to_list()
#     )
    
#     neg_data = []
#     if len(top_items) > 0:
#         # Vector hÃ³a viá»‡c random sampling Ä‘á»ƒ nhanh hÆ¡n
#         for cust_id in tqdm(val_cust_ids, desc="Generating Negatives"):
#             sampled_items = random.sample(top_items, min(len(top_items), n_neg))
#             for item_id in sampled_items:
#                 neg_data.append((cust_id, item_id, 0))
    
#     schema_map = transaction_lf.schema
#     user_dtype = schema_map["customer_id"]
#     item_dtype = schema_map["item_id"]
    
#     neg_df = pl.DataFrame(
#         neg_data, 
#         schema={"customer_id": user_dtype, "item_id": item_dtype, "target": pl.Int64}, 
#         orient="row"
#     )
#     return neg_df

# --- 2. PREDICTION EXPR (DÃ nh cho LightGBM - Native Inference) ---
# LightGBM cÃ³ thá»ƒ dÃ¹ng trees_to_dataframe hoáº·c predict lÃ¡, nhÆ°ng phá»©c táº¡p Ä‘á»ƒ convert sang Polars Expr thuáº§n.
# NÃªn ta sáº½ táº¯t tÃ­nh nÄƒng "Native Inference" vÃ  dÃ¹ng Sklearn fallback (váº«n ráº¥t nhanh vá»›i LGBM).
def get_prediction_expr(model, feature_cols):
    return None # Táº¯t Native Polars mode, dÃ¹ng model.predict()

# --- 3. TRAINING FUNCTION (Switch to LightGBM) ---
def train_model(df_train, feature_cols, model_name, cfg):
    print(f">> Training Model: {model_name} (LightGBM Ranker)...")

    print("   -> Converting Training Data to Numpy (Float32)...")
    df_train = df_train.sort("customer_id")
    try:
        X = df_train.select(feature_cols).fill_null(0).cast(pl.Float32).to_numpy()
        y = df_train.select("target").to_numpy().ravel()
    except:
        X = (
            df_train
            .select(feature_cols)
            .fill_null(0)
            .to_pandas()
            .values
            .astype(np.float32)
        )
        y = df_train.select("target").to_pandas().values.ravel()

    # ===== GROUP (RANKING QUAN TRá»ŒNG NHáº¤T) =====
    print("   -> Building group (by customer_id)...")
    try:
        group = (
        df_train
        .group_by("customer_id", maintain_order=True)
        .len()
        ["len"]
        .to_numpy()
    )
    except:
        group = (
            df_train
            .to_pandas()
            .groupby("customer_id")
            .size()
            .values
        )

    print(f"   -> Total groups: {len(group)}")

    # ===== CONFIG LIGHTGBM RANKER =====
    print("   -> Configuring LightGBM Ranker...")
    model = lgb.LGBMRanker(
        objective="lambdarank",
        metric="ndcg",
        boosting_type="gbdt",
        n_estimators=1000,       # TÄƒng estimator lÃªn vÃ¬ data lá»›n (10M dÃ²ng)
        learning_rate=0.05,      # Learning rate vá»«a pháº£i
        num_leaves=63,           # TÄƒng Ä‘á»™ phá»©c táº¡p cho cÃ¢y
        max_depth=12,            
        min_child_samples=50,    # TrÃ¡nh overfit vá»›i user Ã­t data
        subsample=0.8,           # Row sampling
        colsample_bytree=0.8,    # Feature sampling
        random_state=42,
        n_jobs=-1,
        importance_type='gain',  # DÃ¹ng gain chuáº©n hÆ¡n
        verbose=-1
    )

    print(f"   -> Fitting ranker on {X.shape} matrix...")
    model.fit(X, y, group=group)

    # =====================================================
    # ğŸ“Œ BIAS (BASELINE SCORE â€“ THAM CHIáº¾U)
    # =====================================================
    print("\nğŸ“Œ MODEL BIAS (Baseline Score â€“ Reference):")
    preds = model.predict(X)
    bias = float(np.mean(preds))
    print(f"   -> Mean prediction score: {bias:.6f}")

    # =====================================================
    # ğŸ“Š FEATURE IMPORTANCE
    # =====================================================
    print("\nğŸ“Š FEATURE IMPORTANCE (LightGBM Ranker):")

    booster = model.booster_
    importance = booster.feature_importance(importance_type="gain")

    imp_df = (
        pd.DataFrame({
            "Feature": feature_cols,
            "Importance": importance
        })
        .sort_values(by="Importance", ascending=False)
        .reset_index(drop=True)
    )

    imp_df["Importance_norm"] = (
        imp_df["Importance"] / imp_df["Importance"].sum()
    )

    print(imp_df.to_string(
        index=False,
        formatters={
            "Importance_norm": "{:.4f}".format
        }
    ))

    print("\nğŸ“Œ Gá»¢I Ã DEBUG:")
    print(" - Feature < 1% â†’ DROP")
    print(" - Feature top quÃ¡ máº¡nh â†’ kiá»ƒm tra leakage")
    print(" - Bias tÄƒng nhÆ°ng precision giáº£m â†’ candidate bá»‹ loÃ£ng")

    print("=" * 60)

    return model
